# Models

Store GGUF model files in this directory for benchmarking.  
As an example, use a [4-bit quantized Phi-4-mini-instruct][p].  

[p]: https://huggingface.co/pujeetk/phi-4-mini-instruct-Q4_K_M "phi-4-mini-instruct-Q4_K_M"

## General Setup Instructions (Linux/macOS)
Clone the repository and navigate to the 'models' directory.  
```bash
git clone https://github.com/pkapoor12/magosmaster-chatbot.git
cd magosmaster-chatbot/models/
```

Place the GGUF files for the models you want to benchmark in this directory.  
Using a 4-bit quantized Phi-4-mini-instruct as an example **(don't try to do this download on HPC cluster, instead copy over your GGUF files with scp)**:  
  
*Install and initialize Git LFS first if you haven't already*  
```bash
sudo apt-get install git-lfs
git lfs install
```
*Then, download the model into the models directory*  
```bash
git clone https://huggingface.co/pujeetk/phi-4-mini-instruct-Q4_K_M
mv phi-4-mini-instruct-Q4_K_M/phi-4-mini-instruct-Q4_K_M.gguf .
rm -rf phi-4-mini-instruct-Q4_K_M/
```

Navigate to the 'eval' directory.  
```bash
cd eval/
```  

Create a 'models.txt' containing relative paths (from 'eval' directory) or absolute paths to each model you want to benchmark on each line. Such as:  
```text
../phi-4-mini-instruct-Q4_K_M.gguf  
```
See 'eval/models.txt' for an example.  

### Resource Benchmark Instructions (Linux/macOS)

Ensure your current working directory is the 'eval' directory, then set up a virtual environment and install dependencies.  
```bash
python3 -m venv benchmark_env
source benchmark_env/bin/activate
pip install -r requirements.txt
```

Run the benchmark script
```bash
python3 resource_benchmark.py
```

Results will be saved to 'benchmark_results.json'.  

### Custom Accuracy Benchmark Instructions (Linux/macOS)

Ensure your current working directory is the 'eval' directory, then set up a virtual environment and install dependencies.  
```bash
python3 -m venv benchmark_env
source benchmark_env/bin/activate
pip install -r requirements.txt
```

Create a 'evaluation_set.json' containing question and answer pairs which you want to evaluate the models on. Use the following format:  
```json
[
  {"question": "What is the capital of France?", "answer": "Paris"},
  {"question": "Who wrote Romeo and Juliet?", "answer": "Shakespeare"}
]
```

Run the accuracy benchmark.  
```bash
python3 accuracy_benchmark.py  
```
Results will be saved to 'accuracy_results.json'.  

### MMLU Benchmark Instructions (HPC cluster)

Modify line 18 of mmlu_benchmark.py to set 'n_gpu_layers" to -1 instead of 0 in order to utilize GPU.  
```python
# Initialize evaluator
evaluator = MMLUEvaluation(
     n_ctx=2048,
     n_threads=4,
     n_gpu_layers=-1,  # Set to 35 or more for GPU acceleration
)
```

Submit the MMLU benchmark job script provided to the cluster.  
```bash
sbatch run_mmlu.sh
```
Results will be saved to 'mmlu_results.json' and the .out/.err files generated by the HPC cluster.  